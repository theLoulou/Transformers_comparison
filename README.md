# Transformers_comparison
Comparison of state of the art BERT (Devlin &amp; al, 2018) and its variants DistilBert (Sanh &amp; al, 2019) and ALBERT (Lan &amp; al, 2019)  for text classification using the Transformer package from HuggingFace (https://huggingface.co/transformers/).
The models are evaluated on the IMDB dataset and AG News dataset.

This work was originally done on a local machine and only the final notebook will be put on this repo, without any data.
The comparison of the models was done as a project for the class SYS843 at the ETS.

You can find my final report for this project in the 'articles' directory. It contains in depth explanations of each model, how the comparison was performed, the strengh/weakness of each technique... etc. 

Note that this report and the code in this repo are my work and only represent my opinion based on the results I got. It is not a peer reviewed paper and should not be taken as a completely reliable source. It was a student project with educative and learning purposes and therefore may contain some innacurracies. Check the sources in the 'articles' dir and the ones cited in my final report for reliable peer reviewed information. You should also test the models yourself to ensure it suits your needs.  
